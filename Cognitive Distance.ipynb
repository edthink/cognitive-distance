{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Distance Methods\n",
    "\n",
    "This notebook will accompany the paper 'Towards a Geospatial Model of Cognitive Distance in Urban Spaces: An Objective Measure of Navigibility' by Ed Manley, Gabriele Filomena, and Panos Mavros.\n",
    "\n",
    "The notebook contains the methods used to generate estimates of cognitive distance from conventional GIS data. It also demonstrates the implementation of these methods in estimating cognitive distance in 16 cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import math\n",
    "import requests\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from functools import partial\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import shape\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "ox.config(log_file=True, log_console=True, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urbanFormPy as up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "Each method below simulates the effect of one or more types of cognitive bias in distance perception, as highlighted in the paper. The methods are implemented later in generating estimates for case study cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmark(city, road_network, distance_from_center = 1000):\n",
    "    \n",
    "    # return GeoDataFrame with buildings and landmark scores\n",
    "    \n",
    "    buildings_gdf = up.get_buildings_from_OSM([city], distance_from_center)[0] #works with list of cities\n",
    "    \n",
    "    ox.projection.project_gdf(road_network)\n",
    "    # structural score\n",
    "    buildings_gdf = up.structural_score(buildings_gdf, road_network, max_expansion_distance = 300, \n",
    "                                            distance_along = 50, radius = 150)\n",
    "\n",
    "    # visual score: not really computed at the moment as in most of the cases height information is not provided in OSM\n",
    "    buildings_gdf = up.visual_score(buildings_gdf)\n",
    "    \n",
    "    # culturalscore:\n",
    "    buildings_gdf = up.cultural_score_from_OSM(buildings_gdfs)\n",
    "    \n",
    "    # pragmatic score\n",
    "    \"\"\"\n",
    "    The \"radius\" parameter indicates the extension of the area that is used to compute the score of a building, on the basis of\n",
    "    its land-use's frequency, in an area of 'buffer' meters around it.\n",
    "    \"\"\"\n",
    "    buildings_gdf = up.classify_lu(buildings_gdfs, 'land_use') #check function for classification methodology\n",
    "    buildings_gdf = up.pragmatic_score(buildings_gdf, radius = 200)\n",
    "    \n",
    "    # global landmarkness components weights\n",
    "    g_cW = {'vScore': 0.00, 'sScore' : 0.60, 'cScore': 0.10, 'pScore': 0.30}\n",
    "    # global landmarkness indexes weights, cScore and pScore have only 1 index each\n",
    "    g_iW = {'3dvis': 0.00, 'fac': 0.00, 'height': 0.00, 'area': 0.25, '2dvis': 0.35, 'neigh': 0.20 , 'road': 0.20}\n",
    "    buildings_gdf = up.compute_global_score(buildings_gdf, g_cW, g_iW)\n",
    "    \n",
    "    return buildings_gdf\n",
    "\n",
    "def landmark_presence(route, buildings, threshold): \n",
    "    \n",
    "    # better to use not rescaled gScore values as different cities are tested\n",
    "    landmarks = buildings[buildings.gScore >= threshold]\n",
    "    \n",
    "    \n",
    "    # if route travelling towards a salient feature then distance to feature is reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elev():\n",
    "\n",
    "    # use API to extract elevation for each node\n",
    "    for _, node in node_geo.iterrows():    \n",
    "        \n",
    "        if 'elev' in node_geo.columns and node_geo.loc[[_], 'elev'].iloc[0] > 0: continue # if elev already extracted then skip\n",
    "        \n",
    "        x, y = node['geometry'].x, node['geometry'].y\n",
    "        r_str = 'https://api.open-elevation.com/api/v1/lookup?locations=%s,%s' % (y, x)  # as of 11.2019, API unreliable\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(r_str)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print ('Request error', e)\n",
    "            continue\n",
    "        \n",
    "        elev = 0\n",
    "        if r.status_code == 200:\n",
    "            elev = int(r.json()['results'][0]['elevation'])\n",
    "        else:\n",
    "            print (r_str)\n",
    "            print (_, 'elevation not found')\n",
    "        \n",
    "        node_geo.loc[[_],  'elev'] = elev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope(threshold, pc_inc):\n",
    "    # adjust distance where variation in elevation between u and v\n",
    "    \n",
    "    for _, road in road_geo.iterrows():\n",
    "        \n",
    "        start_elev = node_geo['elev'].loc[[road['u']]].values[0]\n",
    "        end_elev = node_geo['elev'].loc[[road['v']]].values[0]\n",
    "        \n",
    "        slope = ((start_elev - end_elev) / road['length']) * 100\n",
    "      \n",
    "        if slope > threshold or slope < -threshold:\n",
    "            #print (road['name'], slope)\n",
    "            curr_distance = road_geo['new_length'].loc[[_]].values[0]\n",
    "            road_geo.loc[[_],  'new_length'] = curr_distance * pc_inc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Land Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def land_use(buff_dist, pc_inc):\n",
    "    # proximity to shops/activities leads to distraction and distance overestimation\n",
    "    \n",
    "    road_geo_buff = road_geo.copy()\n",
    "    road_geo_buff = road_geo_buff.to_crs({'init': city_epsg})\n",
    "    road_geo_buff['geometry'] = road_geo_buff.apply(buffer, args=[buff_dist], axis=1) # ~25m buffer set up around roads\n",
    "    \n",
    "    for _, road in road_geo_buff.iterrows():\n",
    "        features = 0\n",
    "        features += buildings.loc[buildings['amenity'].notna()].intersects(road['geometry']).values.sum()\n",
    "        \n",
    "        if features >= 3: \n",
    "            curr_distance = road_geo['new_length'].loc[[_]].values[0]\n",
    "            road_geo.loc[[_],  'new_length'] = curr_distance * pc_inc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersections and Turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersections(route):\n",
    "    # get junction-based features of the route\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    p3 = []\n",
    "\n",
    "    minor_junc = 0\n",
    "    medium_junc = 0\n",
    "    major_junc = 0\n",
    "    roundabouts = 0\n",
    "    \n",
    "    turns = 0\n",
    "    sum_deviation = 0\n",
    "        \n",
    "    i = 0\n",
    "    \n",
    "    for _, row in route.iterrows():  \n",
    "                \n",
    "        # counts of roundabouts\n",
    "        if node_geo.loc[[row['v']]].highway.iloc[0] == 'mini_roundabout' or node_geo.loc[[row['v']]].highway.iloc[0] == 'turning_circle':\n",
    "            roundabouts += 1\n",
    "        \n",
    "        # get road grades at junction\n",
    "        if len(route) > i+1: connector = route.iloc[[i+1]] # gets next road\n",
    "        else: i+=1; continue\n",
    "        \n",
    "        if connector.name.iloc[0] != row[8]: # if change in road\n",
    "            if connector['highway'].iloc[0] == 'motorway' or connector['highway'].iloc[0] == 'trunk' or connector['highway'].iloc[0] == 'trunk_link':\n",
    "                major_junc += 1\n",
    "            elif connector['highway'].iloc[0] == 'primary':\n",
    "                medium_junc += 1\n",
    "            elif connector['highway'].iloc[0] == 'secondary':\n",
    "                minor_junc += 1\n",
    "        i+=1  \n",
    "        \n",
    "        # turns and total angular deviation\n",
    "        long = row['geometry'].xy[0][0]\n",
    "        lat = row['geometry'].xy[1][0]\n",
    "        p3 = [long, lat]\n",
    "\n",
    "        if p2 == [] or p1 == p2:\n",
    "            p2 = p3\n",
    "            if p1 == []:\n",
    "                p1 = p2\n",
    "            continue\n",
    "\n",
    "        # Convert the points to numpy latitude/longitude radians space\n",
    "        a = np.radians(np.array(p1))\n",
    "        b = np.radians(np.array(p2))\n",
    "        c = np.radians(np.array(p3))\n",
    "\n",
    "        avec = a - b\n",
    "        cvec = c - b\n",
    "\n",
    "        lat = b[0]\n",
    "        avec[1] *= math.cos(lat)\n",
    "        cvec[1] *= math.cos(lat)\n",
    "\n",
    "        angle2deg = angle_between_vectors_degrees(avec, cvec)\n",
    "        deviation = 180-angle2deg\n",
    "        sum_deviation += deviation\n",
    "\n",
    "        p1 = p2\n",
    "        p2 = p3\n",
    "        \n",
    "        if deviation > 60:\n",
    "            turns +=1\n",
    "            \n",
    "    return minor_junc, medium_junc, major_junc, roundabouts, turns, sum_deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_calcs(route, origin, destination):\n",
    "    # get length-based features of route\n",
    "    length = 0\n",
    "    cog_length = 0\n",
    "    euclid = 0\n",
    "        \n",
    "    # get total length\n",
    "    for _, row in route.iterrows():  \n",
    "        cog_length += row['new_length']\n",
    "        length += row['length']\n",
    "\n",
    "    # get geoms and convert to coordinate system to handle metres easily\n",
    "    o_geom = node_geo.loc[[origin]].geometry\n",
    "    d_geom = node_geo.loc[[destination]].geometry\n",
    "    o_geom = o_geom.to_crs({'init': city_epsg})\n",
    "    d_geom = d_geom.to_crs({'init': city_epsg})\n",
    "    \n",
    "    # get euclidean distance\n",
    "    euclid = o_geom.iloc[0].distance(d_geom.iloc[0])\n",
    "    \n",
    "    return length, cog_length, euclid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_complexity(route):\n",
    "    # sum of nodes within radius of each node on route\n",
    "\n",
    "    sum_node = 0\n",
    "    \n",
    "    for _, row in route.iterrows():  \n",
    "        search_geom = node_geo.loc[[row['v']]].geometry\n",
    "        search_geom = search_geom.to_crs({'init': city_epsg})\n",
    "        search_geom = search_geom.buffer(buffer_dist) # make buffer region at end of segment\n",
    "        sum_node += node_geo.intersects(search_geom.unary_union).values.sum() - 1 # search and sum other nodes (subtract 1 for current node)\n",
    "\n",
    "    if sum_node > 0: return sum_node\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route Prominence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prominence(mode):\n",
    "    # relative to travel speed\n",
    "    \n",
    "    for _, road in road_geo.iterrows():\n",
    "        if mode == 'drive':\n",
    "            if road['highway'] == 'motorway' or road['highway'] == 'trunk':\n",
    "                curr_distance = road_geo['new_length'].loc[[_]].values[0]\n",
    "                road_geo.loc[[_],  'new_length'] = curr_distance * 0.7\n",
    "            if road['highway'] == 'primary':\n",
    "                curr_distance = road_geo['new_length'].loc[[_]].values[0]\n",
    "                road_geo.loc[[_],  'new_length'] = curr_distance * 0.8\n",
    "            if road['highway'] == 'secondary':\n",
    "                curr_distance = road_geo['new_length'].loc[[_]].values[0]\n",
    "                road_geo.loc[[_],  'new_length'] = curr_distance * 0.9\n",
    "            if road['highway'] == 'footway' or road['highway'] == 'service':\n",
    "                curr_distance = road_geo['new_length'].loc[[_]].values[0]\n",
    "                road_geo.loc[[_],  'new_length'] = curr_distance * 2.0\n",
    "        elif mode == 'walk':\n",
    "            if road['highway'] == 'footway' or road['highway'] == 'service':\n",
    "                curr_distance = road_geo['new_length'].loc[[_]].values[0]\n",
    "                road_geo.loc[[_],  'new_length'] = curr_distance * footpath_effect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global City Analysis\n",
    "\n",
    "This section implements the methods for estimating cognitive distance and applies them in 16 cities. This process is divided into four steps:\n",
    "\n",
    "1. Setup parameters for the study - including defining the effect of each spatial factor, the test settings, and the location of test cities.\n",
    "2. Iterate over the study cities, loading the road network and buildings for the study region and making initial 'road-based' adjustments to the GIS data.\n",
    "3. In `execute_paths`, randomly select origin and destination nodes, and request Euclidean distances, metric path distances, and cognitive distances.\n",
    "4. All path-based calculations are made in `calculate_distance`, where the majority of 'route-based' adjustments are executed. \n",
    "\n",
    "Additional cities can be tested by adding them to the `test_cities.txt` file, maintaining the same format structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate parameters\n",
    "These current settings are in line with those described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footpath_effect = 0.9\n",
    "land_use_effect = 1.2\n",
    "slope_effect = 1.5\n",
    "minor_junc_effect = 0.01\n",
    "medium_junc_effect = 0.02\n",
    "major_junc_effect = 0.05\n",
    "turn_effect = 0.1\n",
    "config_effect = 0.005\n",
    "near_prox_effect = 0.8\n",
    "near_prox_threshold = 200\n",
    "far_prox_effect = 1.2\n",
    "far_prox_threshold = 800\n",
    "\n",
    "buffer_dist = 25 # buffer distance (in metres) used for proximity calculations\n",
    "slope_threshold = 5.0 # absolute change in slope required for effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_paths = 10 # number of paths to test per city\n",
    "test_radius = 1000 # radius (in metres) of network from city centroid\n",
    "include_elev = False # option to include elevation in calcs\n",
    "travel_mode = 'walk' # 'drive' or 'walk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test cities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_file_path = 'input/test_cities.txt' # location of city centroids and EPSG information\n",
    "cities = pd.read_csv(cities_file_path, delimiter='\\t') # load cities data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run analysis on cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City - Tribeca\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'road_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-d825a398b3d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mpass\u001b[0m \u001b[1;31m# to ignore TopologyException\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mroad_network\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplify_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroad_network\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mbuildings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'init'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcity_epsg\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'road_network' is not defined"
     ]
    }
   ],
   "source": [
    "all_records = []\n",
    "for _, row in cities.iterrows():\n",
    "\n",
    "    city_epsg = 'epsg:' + str(row['epsg'])\n",
    "    print (row['City'])\n",
    "\n",
    "    # get the road network and buildings for given city at given radius from centre point\n",
    "    try:\n",
    "        road_network = ox.graph_from_point([row['Lat'], row['Lon']], distance = test_radius, network_type=travel_mode, simplify=False, clean_periphery=True)\n",
    "        \n",
    "#         buildings = ox.footprints.footprints_from_point([row['Lat'], row['Lon']], distance = test_radius, retain_invalid=False)\n",
    "    except:\n",
    "        pass # to ignore TopologyException \n",
    "    \n",
    "    road_network = ox.simplify_graph(road_network)\n",
    "#     buildings = buildings.to_crs({'init': city_epsg}) #assigned within landmark function\n",
    "    \n",
    "    # extract geography - both nodes and roads\n",
    "    node_geo, road_geo = ox.graph_to_gdfs(road_network, nodes=True, edges=True, fill_edge_geometry=True)\n",
    "    node_geo = node_geo.to_crs({'init': city_epsg})\n",
    "    road_geo = road_geo.to_crs({'init': city_epsg})\n",
    "    road_geo['new_length'] = road_geo['length']\n",
    "    # assigning landmarks scores\n",
    "    buildings = detect_landmarks(row['City'], road_geo, test_radius)\n",
    "    \n",
    "    # apply weights to road segments\n",
    "    if include_elev:\n",
    "        print ('  Extracting elevation')\n",
    "        elev() # calculates elevations of each node using API\n",
    "        print ('  Calculating slope')\n",
    "        slope(slope_threshold, slope_effect) # calculates slope and increases distance above threshold\n",
    "    print ('  Making land use adjustments')\n",
    "    land_use(buffer_dist, land_use_effect) # increases distance where there are intervening opps\n",
    "    print ('  Adjusting according to road grade')\n",
    "    prominence(travel_mode) # adjust length of major roads\n",
    "    \n",
    "    print ('  Executing paths')\n",
    "    city_records = execute_paths(row['City'], city_epsg)\n",
    "    \n",
    "    if len(all_records) == 0: all_records = city_records[:]\n",
    "    else: all_records.extend(city_records)\n",
    "\n",
    "# put all records into a df\n",
    "city_distance_calcs = pd.DataFrame(all_records, columns=['city','euclid','network','cognitive','f_segment','f_isections','f_turns','f_config','f_locations'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Origin-destination selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_paths(this_city, epsg): \n",
    "    \n",
    "    city_records = []\n",
    "    city_count = 0\n",
    "    while city_count < n_paths:\n",
    "\n",
    "        start = road_network.nodes[int(node_geo.sample(1)['osmid'].iloc[0])]\n",
    "        start_nx = start['osmid']\n",
    "        end = road_network.nodes[int(node_geo.sample(1)['osmid'].iloc[0])]\n",
    "        end_nx = end['osmid']\n",
    "\n",
    "        # reproject points to local reference\n",
    "        p1 = Point(start['x'], start['y'])\n",
    "        p2 = Point(end['x'], end['y'])\n",
    "        project = partial(pyproj.transform, \n",
    "            pyproj.Proj(init = 'epsg:4326'), # source coordinate system (WGS84)\n",
    "            pyproj.Proj(init = city_epsg)) # destination coordinate system \n",
    "\n",
    "        start_proj = transform(project, p1)  # apply projection\n",
    "        end_proj = transform(project, p2)  # apply projection\n",
    "        \n",
    "        # calculate distances\n",
    "        euclid_dist = start_proj.distance(end_proj)\n",
    "        cognitive_dist, network_dist, f1, f2, f3, f4, f5 = calculate_distance(start_nx, end_nx)\n",
    "        \n",
    "        if cognitive_dist > -1 and network_dist > -1: # error check\n",
    "            city_records.append([this_city, euclid_dist, network_dist, cognitive_dist, f1, f2, f3, f4, f5])\n",
    "            city_count +=1\n",
    "        \n",
    "        # else: repeat for another OD pair\n",
    "   \n",
    "    return city_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path and distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(start_nx, end_nx):\n",
    "    \n",
    "    try:\n",
    "        metric_route = nx.dijkstra_path(road_network, start_nx, end_nx, weight='length')\n",
    "        cognitive_route = nx.dijkstra_path(road_network, start_nx, end_nx, weight='new_length')\n",
    "    except:\n",
    "        print ('   ERROR: Path computation failure')\n",
    "        return -1, -1, -1, -1, -1, -1, -1\n",
    "\n",
    "    # metric length\n",
    "    route_nodes = list(zip(metric_route[:-1], metric_route[1:]))\n",
    "    index = [road_geo[(road_geo['u']==u) & (road_geo['v']==v)].index[0] for u, v in route_nodes]\n",
    "    route_geo = road_geo.loc[index]\n",
    "    metric_length, _, _ = length_calcs(route_geo, start_nx, end_nx) \n",
    "\n",
    "    # cognitive length\n",
    "    route_nodes = list(zip(cognitive_route[:-1], cognitive_route[1:]))\n",
    "    index = [road_geo[(road_geo['u']==u) & (road_geo['v']==v)].index[0] for u, v in route_nodes]\n",
    "    route_geo = road_geo.loc[index]\n",
    "    _, cog_length, euclid = length_calcs(route_geo, start_nx, end_nx)\n",
    "    \n",
    "    f1 = cog_length - metric_length  #  road segment effects\n",
    "    cog_len_prev = cog_length\n",
    "    \n",
    "    # intersections and turns \n",
    "    minor_junc, medium_junc, major_junc, roundabouts, turns, sum_deviation = intersections(route_geo)\n",
    "    \n",
    "    cog_length *= 1 + (minor_junc * minor_junc_effect)\n",
    "    cog_length *= 1 + (medium_junc * medium_junc_effect)\n",
    "    cog_length *= 1 + ((major_junc + roundabouts) * major_junc_effect)\n",
    "    f2 = cog_length - cog_len_prev  # intersection effect\n",
    "    cog_len_prev = cog_length\n",
    "    \n",
    "    cog_length *= 1 + (turns * turn_effect)\n",
    "    f3 = cog_length - cog_len_prev  # turn effect\n",
    "    cog_len_prev = cog_length\n",
    "\n",
    "    # network complexity\n",
    "    sum_nodes = network_complexity(route_geo)\n",
    "    cog_length *= 1 + (sum_nodes * config_effect)\n",
    "    f4 = cog_length - cog_len_prev  # network effect\n",
    "    cog_len_prev = cog_length\n",
    "\n",
    "    # context specific relating to proximity of destination\n",
    "    if euclid < near_prox_threshold: cog_length *= near_prox_effect\n",
    "    elif euclid > far_prox_threshold: cog_length *= far_prox_effect\n",
    "    f5 = cog_length - cog_len_prev  # distribution effect\n",
    "\n",
    "    return cog_length, metric_length, f1, f2, f3, f4, f5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation and Export\n",
    "\n",
    "Some simple methods for summarising and exporting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame()\n",
    "summary_table['euclid'] = city_distance_calcs.groupby('city')['euclid'].mean()\n",
    "summary_table['network'] = city_distance_calcs.groupby('city')['network'].mean()\n",
    "summary_table['cognitive'] = city_distance_calcs.groupby('city')['cognitive'].mean()\n",
    "summary_table['f_segment'] = city_distance_calcs.groupby('city')['f_segment'].mean()\n",
    "summary_table['f_isections'] = city_distance_calcs.groupby('city')['f_isections'].mean()\n",
    "summary_table['f_turns'] = city_distance_calcs.groupby('city')['f_turns'].mean()\n",
    "summary_table['f_config'] = city_distance_calcs.groupby('city')['f_config'].mean()\n",
    "summary_table['f_locations'] = city_distance_calcs.groupby('city')['f_locations'].mean()\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'output/results_n' + str(n_paths) + '_r' + str(test_radius) + '_' + travel_mode + '.csv'\n",
    "summary_table.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer(row, value):\n",
    "     return row.geometry.buffer(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between_vectors_degrees(u, v):\n",
    "    try:\n",
    "        angle = np.degrees(math.acos(np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))))\n",
    "        return angle\n",
    "    except:\n",
    "        print ('   WARNING: No angle between vectors detected')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_features(a,b):\n",
    "    return features.loc[[a]].geometry.centroid.iloc[0].distance(features.loc[[b]].geometry.centroid.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
